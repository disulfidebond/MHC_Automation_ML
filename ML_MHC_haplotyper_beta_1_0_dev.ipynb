{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions: Follow the instructions provided in each step, or in the output from a cell \n",
    "Step 1\n",
    "* Make sure the python notebook and the pivot table Excel file are in the **same** folder on your computer.\n",
    "* Enter the name of the pivot table in the next cell, then press the 'run' button above.\n",
    "* The python notebook will give you a preview of what will be used in the analysis beneath the cell after you press the run button.  \n",
    "  * Scan the row names to verify that they are the same as in the Excel sheet that you want ot use.  If they are, skip the following cells and proceed to Step 2.\n",
    "* Otherwise, follow the instructions in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "The numer one goal is to have a python notebook than seamlessly runs the analysis in such a way that the inner workings are completely invisible to researchers,\n",
    "and the output is simply but effectively presented to researchers for their use to make analytical decisions, whatever they may be.\n",
    "It is important to note that this mindset is not designed to obfuscate or hide the methodology that is employed, but rather \n",
    "that repeated experience has shown researchers who are presented with an unfamiliar or confusing tool will opt to ignore it, irrespective of its accuracy, utility, or possible effectiveness.\n",
    "\n",
    "This tool has been set up to do exactly that.  On the surface, it mimics the python notebooks that have been utilized by the O'Connor Lab and Genetic Services,\n",
    "and under the hood, it employs a Machine Learning Classifier to predict MHC-A and MHC-B haplotypes for samples. I would strongly encourage any\n",
    "graduate students who are interested in starting to learn Machine Learning, or who have experience with Machine Learning and would like to know more, to \n",
    "begin here. The only difference between the \"_dev\" version is it includes all of my code comments, notes, post-build notes, and future comments \n",
    "on what direction this build will take next.  The Classifier is setup to use Biological concepts, which hopefully will make it familiar.\n",
    "If you wish to use it as a cookbook for parsing tricks with python, that is perfectly acceptable too; at this point, the workflow is setup\n",
    "to parse input data, format it for the Classifier, run the Classifier, and then output the updated file. However, see my comments regarding the future build 1_2.\n",
    "\n",
    "Those that simply wish to run the notebook and do not want to tinker under the hood may use either the \"_dev\" version or the non-dev version; \n",
    "unless specifically noted at the very beginning of the dev file, both versions will mirror each other and be fully functional. The non-dev file will always be a stable, runnable version. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "Terminology:\n",
    "There are two main types of comments. Code comments usually are very specific and brief, \n",
    "and are preceeded by a '#' within a code cell. Usually these are written by John.\n",
    "Developer Comments are code cells, but surrounded by three single quotes.  They are always attributed to an author, \n",
    "denoted by their initials, example JRC = John R Caskey\n",
    "These comments are also more verbose, and can be on topics ranging from a general overview to a discussion of theory\n",
    "\n",
    "Naming Conventions:\n",
    "Alpha builds are effectively developer builds.  They are any python notebook or script with the name \"alpha\" in them,\n",
    "example: ML_MHC_haplotyping_alpha_1_0\n",
    "ML MHC haplotyping pytho notebook, alpha build 1.0\n",
    "\n",
    "Beta builds are builds that are designed for widespread release to the O'Connor Lab and/or Genetics Services.  The versioning is controlled by github (see below)\n",
    "and by the 2 numbers at the end, example:\n",
    "ML_MHC_hapotyper_beta_1_0_dev\n",
    "ML MHC haplotyper beta build, version 1.0, developer notes version\n",
    "\n",
    "There are two caveats with beta builds, beyond the one already mentioned re: \"_dev\" naming.  First, python notebooks with \"prebuild\" in the name\n",
    "are a working version of the upcoming beta version, usually for the version that is denoted in the numbers in the name, and usually for a specific part of that version.\n",
    "Second, in rare cases, the \"_dev\" version--and only the \"_dev\" version--of the current beta build may be out of sync, and may not run as expected, \n",
    "but this will always be noted at the top of that python notebook in these rare cases.\n",
    "\n",
    "GitHub:\n",
    "GitHub will be used for version control of this project.  The current Beta version 1.0 will start as the effective beginning \n",
    "and as the Master branch, with all notes and comments included.  The \"beta\" branch will function as the latest stable branch for download, or LabKey can be used.\n",
    "The \"alpha\" branch is a development branch that should be considered unstable, which only John or someone actively interested in development should ever use.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot_table = '20411_Felber1-2_MHC-I_Haplotypes_23Mar18.xlsx' # enter name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "Due to multiple comments, there is a slight difference between this and the non-dev python notebook: \n",
    "the cell with most/all of the functions has been broken up into multiple cells that need to be run, separated by comments.  \n",
    "In the non-dev version, it is simply one cell to run.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "def parseExcelWithPandas(fName, excelFileP):\n",
    "    eSheetData = ''\n",
    "    dfs = {sheet_name: excelFileP.parse(sheet_name)\n",
    "      for sheet_name in excelFileP.sheet_names}\n",
    "    if not fName:\n",
    "        sheetCount = -1\n",
    "        for s in dfs:\n",
    "            sheetCount += 1\n",
    "            m = re.search('pivot', s)\n",
    "            m2 = re.search('MiSeq', s)\n",
    "            if m:\n",
    "                eSheetDataInt = pd.ExcelFile(pivot_table)\n",
    "                eSheetData = eSheetDataInt.parse(sheetCount)\n",
    "            elif m2:\n",
    "                eSheetDataInt = pd.ExcelFile(pivot_table)\n",
    "                eSheetData = eSheetDataInt.parse(sheetCount)\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        for s in dfs:\n",
    "            m = re.search(fName, s) # case insensitive?\n",
    "            if m:\n",
    "                eSheetDataInt = pd.ExcelFile(pivot_table)\n",
    "                eSheetData = eSheetDataInt.parse(sheetCount)\n",
    "            else:\n",
    "                continue\n",
    "    return eSheetData\n",
    "\n",
    "def findColumnIdxStartStop(pdDF):\n",
    "    xCt = -1\n",
    "    xStart = -1\n",
    "    xStop = -1\n",
    "    foundInitialMatch = False\n",
    "    for x in excelSheetName.columns.values:\n",
    "        xCt += 1\n",
    "        if xCt == 0:\n",
    "            continue\n",
    "        else:\n",
    "            if xCt < 10 and foundInitialMatch == False: # column idx will not be greater than 9\n",
    "                m = re.search('named', str(x))\n",
    "                if m:\n",
    "                    continue\n",
    "                else:\n",
    "                    foundInitialMatch = True\n",
    "                    xStart = xCt\n",
    "            else: \n",
    "                m = re.search('named', str(x))\n",
    "                if m:\n",
    "                    xStop = xCt\n",
    "                    break\n",
    "    return (xStart, xStop)\n",
    "def parsePandasDfRows(col1ListFromPdDf):\n",
    "    headers = True\n",
    "    mamuA_indices = []\n",
    "    mamuB_indices = []\n",
    "    skipIndices = []\n",
    "    genotypeList = []\n",
    "    for idx,val in enumerate(col1ListFromPdDf):\n",
    "        if headers:\n",
    "            m = re.search('Comment', str(val))\n",
    "            mA = re.search('Mamu-A', str(val))\n",
    "            mB = re.search('Mamu-B', str(val))\n",
    "            if m:\n",
    "                headers = False\n",
    "                skipIndices.append(idx)\n",
    "                continue\n",
    "            elif mA:\n",
    "                mamuA_indices.append(idx)\n",
    "            elif mB:\n",
    "                mamuB_indices.append(idx)\n",
    "            else:\n",
    "                skipIndices.append(idx)\n",
    "                continue\n",
    "        else:\n",
    "            m = re.search('Alleles', str(val))\n",
    "            if m:\n",
    "                skipIndices.append(idx)\n",
    "            else:\n",
    "                genotypeList.append(val)\n",
    "                continue\n",
    "    return (skipIndices, mamuA_indices, mamuB_indices, genotypeList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "using pd.read_excel(pivot_table, sheet_name=excelSheetName)\n",
    "is vulnerable to a bug whereby pandas will intermittently ignore parameters and grab the first sheet only, \n",
    "UNLESS the 0-indexed integer for the sheet is specified with the function x = pd.ExcelFile() followed by x.parse(int)\n",
    "Therefore, the roundabout lookup of parsing the name, parsing the sheet ID integer, then parsing the file from the \n",
    "parsed sheet ID integer, was used instead, to guarantee workability with minimal future debugging being required.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataToPandasOneHot(d, pdDf, idxList):\n",
    "    if pdDf is None:\n",
    "        pdDf = pd.DataFrame.from_dict(d, orient='index').transpose()\n",
    "        pdDf.index = idxList\n",
    "        pdDf.index.name = 'genotype'\n",
    "    else:\n",
    "        alleleDFnew = pd.DataFrame.from_dict(d, orient='index').transpose()\n",
    "        alleleDFnew.index = idxList\n",
    "        alleleDFnew.index.name = 'genotype'\n",
    "        alleleDFres = pd.concat([pdDf, alleleDFnew], axis=1, join_axes=[pdDf.index])\n",
    "        pdDf = alleleDFres\n",
    "    return pdDf\n",
    "def parseIdxForMamuAMamuB(gList):\n",
    "    mamuA_nameIdxListCol1 = []\n",
    "    mamuB_nameIdxListCol1 = []\n",
    "    for idx, n in enumerate(gList):\n",
    "        m_MamuA = re.search('Mamu_A', str(n))\n",
    "        m_MamuB = re.search('Mamu_B', str(n))\n",
    "        if m_MamuA:\n",
    "            filterMamu = re.search('Mamu_AG', str(n))\n",
    "            if filterMamu:\n",
    "                continue\n",
    "            else:\n",
    "                mamuA_nameIdxListCol1.append(idx)\n",
    "        elif m_MamuB:\n",
    "            mamuB_nameIdxListCol1.append(idx)\n",
    "        else:\n",
    "            continue\n",
    "    return (mamuA_nameIdxListCol1, mamuB_nameIdxListCol1)\n",
    "\n",
    "def parseGenotypeList(gList):\n",
    "    r = []\n",
    "    for g in gList:\n",
    "        if type(g) is float:\n",
    "            r.append(g)\n",
    "        else:\n",
    "            gString = g.split('_')\n",
    "            gStringAsList = gString[1:]\n",
    "            gStringAsString = '_'.join(gStringAsList)\n",
    "            r.append(gStringAsString)\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "parseGenotypeList() was updated because some rows may have nan values after being added to a pandas dataframe, \n",
    "which is considered a float.  This value is parsed out later in the workflow, but at this point, filtering it out would require \n",
    "redoing a lot more than simply skipping them.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "Here I ran into a problem.  \n",
    "The data is being formatted to match what is required by the Classifier, basically a one-hot vector,\n",
    "but what happens if there's a genotype in the input data that does not exist in the training data?\n",
    "\n",
    "The immediate effect is accuracy tanks: the classifier does not know how to quantify the genotype, \n",
    "any more than a new biology student would know how to classify M. tuberculosis with a Gram stain.\n",
    "\n",
    "There's several solutions here.\n",
    "\n",
    "You could expand the training set: in the example above, show the grad student a microbiology textbook \n",
    "and instruct him or her to look up acid-fast staining. In the current Classifier, this would mean painstakingly \n",
    "iterating through all of the past experiments to ensure as many of the genotypes as possible are listed.\n",
    "Sharp-eyed readers will notice a \"gotcha\" here, however: what happens when a novel allele is discovered and/or registered in the IRD database?\n",
    "\n",
    "One possibility is to update the training model, but this can get cumbersome, and somewhat defeats \n",
    "the purpose of using a Classifier if the model must be continuously updated.\n",
    "\n",
    "If you're thinking 'Can't the grad student (or the Classifier for a genotype name) draw a logical conclusion from the Mycobacterium genus about both bacteria?'\n",
    "The answer is yes.  Definitely, hopefully, yes for the grad student, and yes for the Classifier.  \n",
    "But, the Classifier will need to be more complex.  At the moment, it looks at the results, and draws correlations based on only the results.\n",
    "Other neural networks like Convolutional Neural Networks (CNN) are designed to handle more complex data, and draw more complex conclusions, and the main goal for build 1.2, \n",
    "as in 2 builds from this one, is to create a CNN that will be able to handle unknown genotype names effectively.\n",
    "\n",
    "Another option is to set up a parser of some kind that will scan and catch instances where a genotype name doesn't exist, and then do something.\n",
    "However, a critical constraint is that the workflow MUST run efficiently without errors, or requiring recoding intervention of any kind.\n",
    "\n",
    "What I did for the first build was split the difference, and as noted, I delayed implementing a more complex neural network until later.\n",
    "When building the training set, I assembled all possible genotype names in an attempt to minimize the chance of a name not appearing.\n",
    "Then, I built a parser that functions as follows:\n",
    "1) Entry names in the input dataset are scanned, and compared to the names in the training dataset.\n",
    "2) for each entry name in the input dataset:\n",
    "  if the entry name exists one time in the training dataset:\n",
    "    continue, no action is necessary\n",
    "  else:\n",
    "    grep for that entry name in the training dataset, minus a 'g' character.\n",
    "    if the grep result is successful AND only one grep result occurs:\n",
    "      print a Warning that the entry name in the input dataset will be changed to the entry name in the training dataset that matched to it\n",
    "    else OR if no grep matches occurred:\n",
    "      print a Warning that the entry name in the input dataset will be deleted\n",
    "\n",
    "A few notes on this parser are:\n",
    "* It will stop at the first match and not attempt to resolve conflicts.  Setting up logic in a parser for such a conflict resolution would be prohibitively difficult, \n",
    "and training a ML model is both planned and would be more effective anyway\n",
    "* If the parser finds more than one grep result, this will trigger the same effect as not finding any.  The rationale is similar to above: this is most likely caused by similar but insufficiently unique entry names when comparing the training dataset and the input dataset, but resolving this naming conflict has been \"punted\" on until the next build.\n",
    "* In the event of a naming deletion, this entry will be deleted from the input data. The training data is included but is not modified in any way.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scanAndUpdateGenotypeList(l1_parsed,l2_training):\n",
    "    l1_parsed_res = []\n",
    "    for i in l1_parsed:\n",
    "        didFindMatch = False\n",
    "        if (l2_training.count(i) == 1):\n",
    "            l1_parsed_res.append((1, i))\n",
    "            continue\n",
    "        else:\n",
    "            if type(i) is float:\n",
    "                l1_parsed_res.append((0, i))\n",
    "                # see dev comments\n",
    "                continue\n",
    "            iString = i.split('g')\n",
    "            iStringRgx = iString[0]\n",
    "            for itm in l2_training:\n",
    "                m1 = re.search(iStringRgx, itm)\n",
    "                if m1:\n",
    "                    mString = m1.group(0)\n",
    "                    if (l2_training.count(mString) == 1):\n",
    "                        l1_parsed_res.append((2, mString))\n",
    "                        didFindMatch = True\n",
    "                        print('WARNING: replacing original item ' + str(i) + ' with modified match from training set: ' + str(mString))\n",
    "                        break # this is important: it stops at the first match\n",
    "                else:\n",
    "                    continue\n",
    "            if not didFindMatch:\n",
    "                print('WARNING! Unable to match item ' + str(i))\n",
    "                print('Item ' + str(i) + ' was deleted from dataset.\\nThis behavior will be modified in a future release.')\n",
    "                l1_parsed_res.append((0, i))\n",
    "            else:\n",
    "                continue\n",
    "    return l1_parsed_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testFile = pd.ExcelFile(pivot_table)\n",
    "readyToProceed = False\n",
    "fName_none = ''\n",
    "dataAsPandas = ''\n",
    "excelSheetName = parseExcelWithPandas(fName_none, testFile)\n",
    "processedPivotTable = False\n",
    "if pivot_table:\n",
    "    if excelSheetName.empty:\n",
    "        print('The Excel sheet was found, but there was an error reading the Excel file.')\n",
    "        print('Please do one of the following:\\n\\nProceed to the next cell and attempt to enter the sheet_name value,\\n\\nor\\n\\nExport the file as a csv, start from the beginning of the python notebook,\\nchange the file name, and rerun all cells.\\nBe sure to specify the file type as csv in the cell below when you run it.')\n",
    "    else:\n",
    "        print('Please click the next cell, and press Run.')\n",
    "        print('Here is a preview of the data: \\n\\n##StartPreview:\\n')\n",
    "        pdHeadersRowsCol1 = list(excelSheetName.iloc[1:10:,0])\n",
    "        for h in pdHeadersRowsCol1:\n",
    "            m = re.search('Animal ID', h)\n",
    "            if m:\n",
    "                processedPivotTable = True\n",
    "            print(h + '\\n')\n",
    "        print('##EndPreview\\n\\nData Type is ')\n",
    "        if processedPivotTable:\n",
    "            print('Processed Pivot Table')\n",
    "        else:\n",
    "            print('raw pivot table')\n",
    "        readyToProceed = True\n",
    "else:\n",
    "    print('No pivot table was found. Please do the following: \\n1) check the filename and rerun the previous cell\\n2) if you have already rerun the previous cell and are seeing this message again, \\nproceed to the next cell and enter information for at least one of the following: ')\n",
    "    print('\\tfile_type: enter \\'csv\\' or \\'excel\\', depending on the file.')\n",
    "    print('\\tsheet_name: enter the sheet name for the excel spreadsheet or csv file')\n",
    "    print('Then run the next two cells.')\n",
    "\n",
    "genotypeList_training = openFileAsList('alleles_parsed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "genotypeList_training is the genotype names list that is used later, and is commented on as well in a later comment\n",
    "The file was created with a bash script that extracted the genotype names, sorted them by unique values, then concatenated the list for all MiSeq output files, \n",
    "and then repeated the sort/unique step one more time\n",
    "See the file 'workflow_comments_filterParse.md' in the alpha branch for detailed notes\n",
    "\n",
    "See the \"ToDo\" task list at the end as well.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_type = '' # must be left blank, 'csv' for a csv file, or 'excel' for an excel file type\n",
    "sheet_name = '' # must have the excel sheet name with the pivot table, or be blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "* This step will reformat the Excel data into a format that can be used by the Machine Learning Classifier.\n",
    "* Click the next cell, and then click the Run button at the top.\n",
    "* If you do not see any error messages, and you see the output 'Everything looks good!', then proceed to Step 3.  Otherwise, contact John for assistance with Step 2\n",
    "  * Note from John: In the next Beta build (1.2) this step will be modified to not require intervention from me if something goes wrong.\n",
    "* If you see a warning message, it is still usually ok to proceed, but you should make a note of the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseLabeledValues(l):\n",
    "    r = []\n",
    "    for i in l:\n",
    "        iSplit = i.split('-')\n",
    "        iSplitSorted = sorted(iSplit)\n",
    "        iSplitSortdString = '-'.join(iSplitSorted)\n",
    "        if iSplit[0] == iSplit[1]:\n",
    "            r.append(iSplit[0])\n",
    "        else:\n",
    "            r.append(iSplitSortdString)\n",
    "            # this is better approach than checking if the reverse string is present\n",
    "            # sort all pairs and append, since duplicates do not matter, but order does\n",
    "            # IMPORTANT: this does NOT remove any entries, only modifies labeling\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comment: JRC\n",
    "The above function will be incorporated into tool described in the Dev Comments.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = findColumnIdxStartStop(excelSheetName)\n",
    "if v[0] == -1:\n",
    "    print('Error!  check dataframe!')\n",
    "col1List = list(excelSheetName['Sample Sheet #'])\n",
    "skipRows, mamuA_rows, mamuB_rows, genotypeList_unparsed = parsePandasDfRows(col1List)\n",
    "genotypeList_parsed = parseGenotypeList(genotypeList_unparsed)\n",
    "genotypeListTuple = scanAndUpdateGenotypeList(genotypeList_parsed, genotypeList_training)\n",
    "# final step: parse through genotypeListTuple, and remove any rows that correspond to (0, name)\n",
    "genotypeListFiltered = list(filter(lambda x: x[0] != 0, genotypeListTuple))\n",
    "genotypeListTupleX, genotypeListTupleY = zip(*genotypeListTuple) # zip iterable, unpacks tuple\n",
    "genotypeListSkip = [x for (x,y) in enumerate(genotypeListTupleX) if y == 0]\n",
    "genotypeIntsTuple, genotypeListFromTuple = zip(*genotypeListFiltered) \n",
    "genotypeList = list(genotypeListFromTuple)\n",
    "skipMamuRows = mamuA_rows + mamuB_rows + skipRows + genotypeListSkip\n",
    "skipMamuRows.sort()\n",
    "parsedMamuIndices = parseIdxForMamuAMamuB(genotypeList)\n",
    "parsedMamuAIndices = parsedMamuIndices[0]\n",
    "parsedMamuBIndices = parsedMamuIndices[1]\n",
    "\n",
    "# print(genotypeList)\n",
    "rStart = v[0]\n",
    "rStop = v[1] + 1 \n",
    "rRange = rStop - rStart\n",
    "mamuA_alleleList = []\n",
    "mamuB_alleleList = []\n",
    "alleleDF_MamuA = None\n",
    "alleleDF_MamuB = None\n",
    "for x in range(rStart, rStop):\n",
    "    mamu_genotypes_oneHot = []\n",
    "    dfValue = excelSheetName.iloc[:,x] # verify syntax for rows and columns, and index not lookup\n",
    "    dfValue_MamuA_1 = dfValue.iloc[int(mamuA_rows[0])]\n",
    "    dfValue_MamuA_2 = dfValue.iloc[int(mamuA_rows[1])]\n",
    "    dfValue_MamuB_1 = dfValue.iloc[int(mamuB_rows[0])]\n",
    "    dfValue_MamuB_2 = dfValue.iloc[int(mamuB_rows[1])]\n",
    "    pdDict_MamuA = dict()\n",
    "    pdDict_MamuB = dict()\n",
    "    for idx, row in dfValue.iteritems():\n",
    "        if idx not in skipMamuRows:\n",
    "            try:\n",
    "                if math.isnan(row):\n",
    "                    mamu_genotypes_oneHot.append(0)\n",
    "                    continue\n",
    "                else:\n",
    "                    mamu_genotypes_oneHot.append(1)\n",
    "            except TypeError:\n",
    "                mamu_genotypes_oneHot.append(1)\n",
    "    pdDictKey_MamuA = str(dfValue_MamuA_1) + '-' + str(dfValue_MamuA_2)\n",
    "    pdDictKey_MamuB = str(dfValue_MamuB_1) + '-' + str(dfValue_MamuB_2)\n",
    "    pdDict_MamuA[pdDictKey_MamuA] = mamu_genotypes_oneHot\n",
    "    pdDict_MamuB[pdDictKey_MamuB] = mamu_genotypes_oneHot\n",
    "    alleleDF_MamuA = dataToPandasOneHot(pdDict_MamuA, alleleDF_MamuA, genotypeList)\n",
    "    alleleDF_MamuB = dataToPandasOneHot(pdDict_MamuB, alleleDF_MamuB, genotypeList)\n",
    "alleleDF_MamuA_parsed = alleleDF_MamuA.iloc[parsedMamuAIndices,:]\n",
    "alleleDF_MamuB_parsed = alleleDF_MamuB.iloc[parsedMamuBIndices,:]\n",
    "alleleDF_MamuA_parsedList = []\n",
    "alleleDF_MamuA_listLabels_Y = []\n",
    "for i in range(0, rRange):\n",
    "    dfColAsSeries = alleleDF_MamuA_parsed.iloc[:,i]\n",
    "    if dfColAsSeries.name == 'nan-nan':\n",
    "        continue\n",
    "    dfColAsList = dfColAsSeries.tolist()\n",
    "    alleleDF_MamuA_parsedList.append(dfColAsList)\n",
    "    alleleDF_MamuA_listLabels_Y.append(dfColAsSeries.name)\n",
    "alleleDF_MamuA_listedLabels_Y = parseLabeledValues(alleleDF_MamuA_listLabels_Y)\n",
    "alleleDF_MamuA_parsedNP = np.array(alleleDF_MamuA_parsedList)\n",
    "if not alleleDF_MamuA_parsed.empty and not alleleDF_MamuB_parsed.empty:\n",
    "    print('Everything looks good!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remaining steps todo before release of beta 1_0:\n",
    "# write parser for alleleDF_MamuA_listLabels_Y to merge duplicated and reversed-duplicated values\n",
    "# write function to clean up code to handle MamuA and MamuB above\n",
    "# write logic to handle 2 conditions: labels for formatted pivot table and matching for raw pivot table\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3\n",
    "* If you did not encounter any errors previously, or the output did not direct you to stop, then proceed with the analysis.\n",
    "* You can either click this cell, select the Cell menu above, and then select 'Run All below', or click the cells after this one and individually click 'Run' for each one\n",
    "* The analysis within this python notebook will do the following:\n",
    "  * parse out genotype ID's for the MHC-A, and MHC-B data, \n",
    "  * use a pre-trained model for a Machine Learning Classifier to predict the Haplotype for each sample for MHC-A and MHC-B\n",
    "  * add the predicted values back into the pivot table (or create a new pivot table)\n",
    "  * output the resulting pivot table\n",
    "* Note that for beta 1.x builds, only MHC-A and MHC-B haplotypes will be predicted.  All other haplotypes will be passed to the researcher for analysis.  A researcher should also verify the output pivot table from the Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def lambdaFunc(v):\n",
    "    return int(v)\n",
    "\n",
    "def openAndParse(f):\n",
    "    listValue = []\n",
    "    headerLine = ''\n",
    "    start = True\n",
    "    with open(f) as fOpen:\n",
    "        for i in fOpen:\n",
    "            if start:\n",
    "                iLine = i.split(',')\n",
    "                iLine = iLine.pop(0)\n",
    "                headerLine = ','.join(iLine)\n",
    "                start = False\n",
    "            else:\n",
    "                i = i.rstrip('\\n')\n",
    "                iSplit = i.split(',')\n",
    "                iSplitInt = list(map(lambdaFunc, iSplit[1:]))\n",
    "                # listValue.append((iSplit[0], iSplit[1:]))\n",
    "                listValue.append((iSplit[0], iSplitInt))\n",
    "    return (headerLine, listValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingValues = openAndParse('allele_df-trainingSet-HapA.csv')\n",
    "testingValues = openAndParse('allele_df-testingSet-HapA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comment JRC\n",
    "The testingValues variable and resulting functions will be kept for the time being. Once the training model is set up, the primary use of these will be to run\n",
    "and then \n",
    "if float_testing < 0.92:\n",
    "  print('Warning!  You should not continue, something is wrong!')\n",
    "\n",
    "Or something like that.  However, as I've stressed, the ML model will only be released for testing once it's been tested already rigorously.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.empty([len(trainingValues[1]), len(trainingValues[1][1][1])], dtype=int)\n",
    "Y_train = np.empty([len(trainingValues[1])], dtype=int)\n",
    "X_test = np.empty([len(testingValues[1]), len(testingValues[1][1][1])], dtype=int)\n",
    "Y_test = np.empty([len(testingValues[1])], dtype=int)\n",
    "print(X_test)\n",
    "print(X_test.shape)\n",
    "print('\\n\\n')\n",
    "print(Y_test)\n",
    "print(Y_test.shape)\n",
    "trainingValueFromTuple = trainingValues[1]\n",
    "testingValueFromTuple = testingValues[1]\n",
    "print(testingValueFromTuple)\n",
    "print(len(testingValueFromTuple))\n",
    "lookupTableBecauseNumpy = []\n",
    "lookupTableBecauseNumpyTesting = []\n",
    "stopRange = len(trainingValueFromTuple)\n",
    "for x in range(0, stopRange):\n",
    "    aValue_x = trainingValueFromTuple[x][1]\n",
    "    aValue_y = trainingValueFromTuple[x][0]\n",
    "    X_train[x] = aValue_x\n",
    "    # X_train[x] = aValue_xList\n",
    "    # np.insert(X_train[x], aValue_xList)\n",
    "    lookupTableBecauseNumpy.append(aValue_y)\n",
    "    Y_train[x] = np.array(x, dtype=int)\n",
    "stopRange = len(testingValueFromTuple)\n",
    "for x in range(0, stopRange):\n",
    "    aValue_x = testingValueFromTuple[x][1]\n",
    "    aValue_y = testingValueFromTuple[x][0]\n",
    "    X_test[x] = aValue_x\n",
    "    lookupTableBecauseNumpyTesting.append(aValue_y)\n",
    "    Y_test[x] = np.array(x, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "# PRAGMA MARK: Critical Dev comment\n",
    "\n",
    "A few comments here.\n",
    "At a basic level, the training set creates a correlation between the one-hot vector of values \n",
    "that are fed to it (and was created in this case from the genotypes), with correspinding labeling.  \n",
    "The labeling is important, otherwise the output would be binary, or a series of integers.  Additionally, labeling can be used in downstream\n",
    "tasks and more complex machine learning tasks.\n",
    "\n",
    "There are several algorithms to use for training and prediction, which are too complex to explain here, \n",
    "but will be explained and discussed in later developer comments.\n",
    "The testing set then takes a set of data, which must be formatted as a one-hot vector with corresponding labeling, \n",
    "and then predicts how the test dataset will match to the training dataset.  \n",
    "\n",
    "Initially, testing and training data were from the same pooled data, so creating testing and training data followed similar methods.\n",
    "\n",
    "To use input data, the steps above--commented out now in the dev version, removed in the beta version--needed to be\n",
    "modified such that the input data was predicted to have set values, and then those values were matched to a labeling table.\n",
    "\n",
    "If input data was from a formatted pivot table, then a list of labels are created from the column names.  This becomes Y_test, and accuracy testing can be run.\n",
    "\n",
    "If the input data was from a raw pivot table, then the classifier makes predictions, and these predictions are mapped back to labeling created from the training data set.  \n",
    "In this case, no accuracy checks can be done, since these labels are unknown, however, as has been repeatedly stated, the output should be verified by a researcher.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Developer Comments JRC\n",
    "'''\n",
    "One thing I kept scratching my head over--and then subsequently face-palming over--was why\n",
    "the linear model for the Classifier simply refused to work.\n",
    "\n",
    "As a cautionary tale to anyone else, what was happening was I had correctly set up the X values as a one-hot vector, \n",
    "but then when I set the labels for Y, I assigned each one to a unique integer and then mapped them back to the corresponding string values.\n",
    "This meant the linear model was being set up as \n",
    "X          Y \n",
    "[vector]   0\n",
    "[vector]   1\n",
    "[vector]   2\n",
    "[vector]   3\n",
    "\n",
    "Where each vector was something like:\n",
    "[0,0,0,1,0,0]\n",
    "\n",
    "And each Y integer was unique, and mapped back to a string, like \"A001\".  \n",
    " \n",
    "The vectors were created from the allele names, where a mapping table was created that defined an allele that was present or not for a given haplotype (order was maintained).\n",
    "Each vector represents an instance in an experiment where alleles for a sample mapped back to a haplotype as allele frequencies.  \n",
    "Initially, replicated instances of Haplotypes were flattened and merged, however, I'll leave it as an exercise to the reader to see just how poorly this implementaion fit any linear model. \n",
    "\n",
    "What I should have been doing (and subsequently did) was:\n",
    "X          Y \n",
    "[vector]   0\n",
    "[vector]   0\n",
    "[vector]   0\n",
    "[vector]   1\n",
    "[vector]   1\n",
    "\n",
    "Where again, each vector was something like:\n",
    "[0,0,0,1,0,0]\n",
    "\n",
    "Each Y integer was each instance of the corresponding haplotype, like \"A001\", that mapped to it\n",
    "Subsequently, all allele frequencies were included, regardless if the same allele pairing was repeated.\n",
    "This created a usable linear model, and multiple n values that the library could use for calculations.\n",
    "\n",
    "Another big facepalm was when the Classifier worked flawlessly, but the lookup table referenced above got corrupted, thereby causing\n",
    "0.0% accuracy.  Beyond my checking and verifying the code, and checking the one-hot vectors manually to verify that 0.0% accuracy \n",
    "was a near impossibility, I'll leave it as an exercise to the reader to research to determine why having 0.0% accuracy usually \n",
    "does not occur with Machine Learning, and also why the standard for accuracy in Machine Learning is 99.95% (this number is arbitrarily set, \n",
    "but there is a specific reason why it's set this high).\n",
    "\n",
    "'''\n",
    "# For these purposes, the labels were manually reformatted (see above) by visually scanning the list .  This will be automated.\n",
    "\n",
    "def formatTestingValues(tList, tupleList):\n",
    "    r = []\n",
    "    validationInt = len(tList)\n",
    "    for v in tList:\n",
    "        for vInTuple in tupleList:\n",
    "            if v == vInTuple[0]:\n",
    "                r.append(vInTuple[1])\n",
    "                break\n",
    "    if len(r) != len(tList):\n",
    "        print('WARNING!')\n",
    "        print(r)\n",
    "        print(tList)\n",
    "        return None\n",
    "    else:\n",
    "        return r\n",
    "def valuesToTestList(valList, npArrayTupleListTraining):\n",
    "    r = []\n",
    "    matched = False\n",
    "    for v in valList:\n",
    "        for vTuple in npArrayTupleListTraining:\n",
    "            if v == vTuple[1]:\n",
    "                matched = True\n",
    "                r.append(vTuple[0])\n",
    "                break\n",
    "            # if v == vTuple[0]:\n",
    "                # matched = True\n",
    "                # r.append(vTuple[1])\n",
    "                # break\n",
    "        if matched:\n",
    "            matched = False\n",
    "            continue\n",
    "        else:\n",
    "            print('WARNING!')\n",
    "            print(v)\n",
    "            print(npArrayTupleListTraining)\n",
    "            return None\n",
    "    return r\n",
    "        \n",
    "    \n",
    "def valuesToIntList(valList):\n",
    "    listedValues = []\n",
    "    ct = 0\n",
    "    l_strings = []\n",
    "    l_ints = []\n",
    "    l_strings_r = []\n",
    "    l_ints_r = []\n",
    "    npArrayList = []\n",
    "    for i in valList:\n",
    "        if i not in l_strings:\n",
    "            l_strings_r.append(i)\n",
    "            l_ints_r.append(ct)\n",
    "            l_strings.append(i)\n",
    "            l_ints.append(ct)\n",
    "            npArrayList.append(ct)\n",
    "            ct += 1\n",
    "        else:\n",
    "            v_i = l_strings.index(i)\n",
    "            v_i_string = l_strings[v_i]\n",
    "            v_ct = l_ints[v_i]\n",
    "            l_strings_r.append(v_i_string)\n",
    "            l_ints_r.append(v_ct)\n",
    "            npArrayList.append(v_ct)\n",
    "    npArrayTupleList = list(zip(l_strings_r,l_ints_r))\n",
    "    return (npArrayList, npArrayTupleList)\n",
    "'''\n",
    "listedValues = []\n",
    "ct = 0\n",
    "l_strings = []\n",
    "l_ints = []\n",
    "l_strings_r = []\n",
    "l_ints_r = []\n",
    "print('[')\n",
    "npArrayList = []\n",
    "for i in lookupTableBecauseNumpy:\n",
    "    if i not in l_strings:\n",
    "        print(str(i) + ',' + str(ct))\n",
    "        l_strings_r.append(i)\n",
    "        l_ints_r.append(ct)\n",
    "        # print(str(ct) + ',')\n",
    "        l_strings.append(i)\n",
    "        l_ints.append(ct)\n",
    "        npArrayList.append(ct)\n",
    "        ct += 1\n",
    "    else:\n",
    "        v_i = l_strings.index(i)\n",
    "        v_i_string = l_strings[v_i]\n",
    "        v_ct = l_ints[v_i]\n",
    "        print(str(v_i_string) + ',' + str(v_ct))\n",
    "        l_strings_r.append(v_i_string)\n",
    "        l_ints_r.append(v_ct)\n",
    "        npArrayList.append(v_ct)\n",
    "        # print(str(v_ct) + ',')\n",
    "# print(']')\n",
    "# print(npArrayList)\n",
    "'''\n",
    "\n",
    "parsedTrainingResults = valuesToIntList(lookupTableBecauseNumpy)\n",
    "Y_train = np.array(parsedTrainingResults[0])\n",
    "parsedTestingResults = valuesToIntList(lookupTableBecauseNumpyTesting)\n",
    "Y_testList = valuesToTestList(lookupTableBecauseNumpyTesting, parsedTrainingResults[1])\n",
    "# print(Y_train)\n",
    "Y_test = np.array(Y_testList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"]}]\n",
    "\n",
    "# KNearestNeighbors was used, but a different library may be applied later\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, weights='distance', n_neighbors=4)\n",
    "knn_clf.fit(X_train, Y_train)\n",
    "\n",
    "y_knn_pred = knn_clf.predict(X_test)\n",
    "\n",
    "# forest_clf_pred = forest_clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, y_knn_pred)\n",
    "# This accuracy score strictly tests the model, and may be subject to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, Y_train)\n",
    "forest_clf_pred = forest_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, forest_clf_pred)\n",
    "print(forest_clf_pred)\n",
    "print(Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Developer Comments JRC\n",
    "'''\n",
    "# 1) Cross-validation does exactly what it sounds like: it attempts to validate the results without worrying about overfitting, for a \"truly\" accurate score\n",
    "# 2) This is a bit of a \"hacked\" approach, because it's switching to Stochastic Gradient Descent (SGD) from KNN for the cross validation, but I plan on moving that direction anyway\n",
    "# 3) For the suspicious/worried, you can replace \"sgd_clf\" with \"knn_clf\" or \"forest_clf\" for a random forest lassifier to also review the cross-validation result.\n",
    "# 4) Ignore the warning.  I'm using an outdated module from SciKit-Learn, if I ultimately use TensorFlow or Caffe it won't appear, and if I use SciKit-Learn I'll use an upadated module.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "cross_val_score(sgd_clf, X_train, Y_train, cv=2, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Developer Comments JRC\n",
    "'''\n",
    "This is an ongoing task list for upcoming beta builds.  \n",
    "Note that in the near future, inline developer comments for dev builds will be removed completely, and replaced with versioned Github README files.\n",
    "\n",
    "ToDo list for beta 1_1:\n",
    "* replace openFileAsList() completely with a better parser\n",
    "* additionally, consult with Dave Baker to create a more cohesive data storage unit for training data\n",
    "\n",
    "ToDo list for beta 1_2:\n",
    "* test Stocahstic Gradient Descent (SGD), K-Nearest Neighbors (KNN), and Random Forest Classifier (RF), and decide which algorithm to use for Classifier\n",
    "* test algorithms listed above, and workflow for a modified Natural Language Processing (NLP), based on preliminary proof-of-concept work with fasta headers, \n",
    "to create a ML model that parses genotype names\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
