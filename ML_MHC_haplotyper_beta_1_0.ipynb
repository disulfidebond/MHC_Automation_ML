{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions: Follow the instructions provided in each step, or in the output from a cell \n",
    "Step 1\n",
    "* Make sure the python notebook and the pivot table Excel file are in the **same** folder on your computer.\n",
    "* Enter the name of the pivot table in the next cell, then press the 'run' button above.\n",
    "* The python notebook will give you a preview of what will be used in the analysis beneath the cell after you press the run button.  \n",
    "  * Scan the row names to verify that they are the same as in the Excel sheet that you want ot use.  If they are, skip the following cells and proceed to Step 2.\n",
    "* Otherwise, follow the instructions in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pivot_table = '20411_Felber1-2_MHC-I_Haplotypes_23Mar18.xlsx' # enter name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please click the next cell, and press Run.\n",
      "Here is a preview of the data: \n",
      "\n",
      "##StartPreview:\n",
      "\n",
      "Animal ID\n",
      "\n",
      "# Reads Evaluated\n",
      "\n",
      "# Reads Identified\n",
      "\n",
      "% Unknown\n",
      "\n",
      "Mamu-A Haplotype 1\n",
      "\n",
      "Mamu-A Haplotype 2\n",
      "\n",
      "Mamu-B Haplotype 1\n",
      "\n",
      "Mamu-B Haplotype 2\n",
      "\n",
      "Comments\n",
      "\n",
      "##EndPreview\n",
      "\n",
      "Data Type is \n",
      "Processed Pivot Table\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "\n",
    "def parseExcelWithPandas(fName, excelFileP):\n",
    "    eSheetData = ''\n",
    "    dfs = {sheet_name: excelFileP.parse(sheet_name)\n",
    "      for sheet_name in excelFileP.sheet_names}\n",
    "    if not fName:\n",
    "        sheetCount = -1\n",
    "        for s in dfs:\n",
    "            sheetCount += 1\n",
    "            m = re.search('pivot', s)\n",
    "            m2 = re.search('MiSeq', s)\n",
    "            if m:\n",
    "                eSheetDataInt = pd.ExcelFile(pivot_table)\n",
    "                eSheetData = eSheetDataInt.parse(sheetCount)\n",
    "            elif m2:\n",
    "                eSheetDataInt = pd.ExcelFile(pivot_table)\n",
    "                eSheetData = eSheetDataInt.parse(sheetCount)\n",
    "            else:\n",
    "                continue\n",
    "    else:\n",
    "        for s in dfs:\n",
    "            m = re.search(fName, s) # case insensitive?\n",
    "            if m:\n",
    "                eSheetDataInt = pd.ExcelFile(pivot_table)\n",
    "                eSheetData = eSheetDataInt.parse(sheetCount)\n",
    "            else:\n",
    "                continue\n",
    "    return eSheetData\n",
    "\n",
    "def openFileAsList(f):\n",
    "    l = []\n",
    "    with open(f, 'r') as fOpen:\n",
    "        for i in fOpen:\n",
    "            i = i.rstrip('\\r\\n')\n",
    "            l.append(i)\n",
    "    return l\n",
    "\n",
    "def findColumnIdxStartStop(pdDF):\n",
    "    xCt = -1\n",
    "    xStart = -1\n",
    "    xStop = -1\n",
    "    foundInitialMatch = False\n",
    "    for x in excelSheetName.columns.values:\n",
    "        xCt += 1\n",
    "        if xCt == 0:\n",
    "            continue\n",
    "        else:\n",
    "            if xCt < 10 and foundInitialMatch == False: # column idx will not be greater than 9\n",
    "                m = re.search('named', str(x))\n",
    "                if m:\n",
    "                    continue\n",
    "                else:\n",
    "                    foundInitialMatch = True\n",
    "                    xStart = xCt\n",
    "            else: \n",
    "                m = re.search('named', str(x))\n",
    "                if m:\n",
    "                    xStop = xCt\n",
    "                    break\n",
    "    return (xStart, xStop)\n",
    "def parsePandasDfRows(col1ListFromPdDf):\n",
    "    headers = True\n",
    "    mamuA_indices = []\n",
    "    mamuB_indices = []\n",
    "    skipIndices = []\n",
    "    genotypeList = []\n",
    "    for idx,val in enumerate(col1ListFromPdDf):\n",
    "        if headers:\n",
    "            m = re.search('Comment', str(val))\n",
    "            mA = re.search('Mamu-A', str(val))\n",
    "            mB = re.search('Mamu-B', str(val))\n",
    "            if m:\n",
    "                headers = False\n",
    "                skipIndices.append(idx)\n",
    "                continue\n",
    "            elif mA:\n",
    "                mamuA_indices.append(idx)\n",
    "            elif mB:\n",
    "                mamuB_indices.append(idx)\n",
    "            else:\n",
    "                skipIndices.append(idx)\n",
    "                continue\n",
    "        else:\n",
    "            m = re.search('Alleles', str(val))\n",
    "            if m:\n",
    "                skipIndices.append(idx)\n",
    "            else:\n",
    "                genotypeList.append(val)\n",
    "                continue\n",
    "    return (skipIndices, mamuA_indices, mamuB_indices, genotypeList)\n",
    "def dataToPandasOneHot(d, pdDf, idxList):\n",
    "    if pdDf is None:\n",
    "        pdDf = pd.DataFrame.from_dict(d, orient='index').transpose()\n",
    "        pdDf.index = idxList\n",
    "        pdDf.index.name = 'genotype'\n",
    "    else:\n",
    "        alleleDFnew = pd.DataFrame.from_dict(d, orient='index').transpose()\n",
    "        alleleDFnew.index = idxList\n",
    "        alleleDFnew.index.name = 'genotype'\n",
    "        alleleDFres = pd.concat([pdDf, alleleDFnew], axis=1, join_axes=[pdDf.index])\n",
    "        pdDf = alleleDFres\n",
    "    return pdDf\n",
    "def parseIdxForMamuAMamuB(gList):\n",
    "    mamuA_nameIdxListCol1 = []\n",
    "    mamuB_nameIdxListCol1 = []\n",
    "    for idx, n in enumerate(gList):\n",
    "        m_MamuA = re.search('Mamu_A', str(n))\n",
    "        m_MamuB = re.search('Mamu_B', str(n))\n",
    "        if m_MamuA:\n",
    "            filterMamu = re.search('Mamu_AG', str(n))\n",
    "            if filterMamu:\n",
    "                continue\n",
    "            else:\n",
    "                mamuA_nameIdxListCol1.append(idx)\n",
    "        elif m_MamuB:\n",
    "            mamuB_nameIdxListCol1.append(idx)\n",
    "        else:\n",
    "            continue\n",
    "    return (mamuA_nameIdxListCol1, mamuB_nameIdxListCol1)\n",
    "\n",
    "def parseGenotypeList(gList):\n",
    "    r = []\n",
    "    for g in gList:\n",
    "        if type(g) is float:\n",
    "            r.append(g)\n",
    "        else:\n",
    "            gString = g.split('_')\n",
    "            gStringAsList = gString[1:]\n",
    "            gStringAsString = '_'.join(gStringAsList)\n",
    "            r.append(gStringAsString)\n",
    "    return r\n",
    "\n",
    "def scanAndUpdateGenotypeList(l1_parsed,l2_training):\n",
    "    l1_parsed_res = []\n",
    "    for i in l1_parsed:\n",
    "        didFindMatch = False\n",
    "        if (l2_training.count(i) == 1):\n",
    "            l1_parsed_res.append((1, i))\n",
    "            continue\n",
    "        else:\n",
    "            if type(i) is float:\n",
    "                l1_parsed_res.append((0, i))\n",
    "                # see dev comments\n",
    "                continue\n",
    "            iString = i.split('g')\n",
    "            iStringRgx = iString[0]\n",
    "            for itm in l2_training:\n",
    "                m1 = re.search(iStringRgx, itm)\n",
    "                if m1:\n",
    "                    mString = m1.group(0)\n",
    "                    if (l2_training.count(mString) == 1):\n",
    "                        l1_parsed_res.append((2, mString))\n",
    "                        didFindMatch = True\n",
    "                        print('WARNING: replacing original item ' + str(i) + ' with modified match from training set: ' + str(mString))\n",
    "                        break # this is important: it stops at the first match\n",
    "                else:\n",
    "                    continue\n",
    "            if not didFindMatch:\n",
    "                print('WARNING! Unable to match item ' + str(i))\n",
    "                print('Item ' + str(i) + ' was deleted from dataset.\\nThis behavior will be modified in a future release.')\n",
    "                l1_parsed_res.append((0, i))\n",
    "            else:\n",
    "                continue\n",
    "    return l1_parsed_res\n",
    "    \n",
    "testFile = pd.ExcelFile(pivot_table)\n",
    "readyToProceed = False\n",
    "fName_none = ''\n",
    "dataAsPandas = ''\n",
    "excelSheetName = parseExcelWithPandas(fName_none, testFile)\n",
    "processedPivotTable = False\n",
    "if pivot_table:\n",
    "    if excelSheetName.empty:\n",
    "        print('The Excel sheet was found, but there was an error reading the Excel file.')\n",
    "        print('Please do one of the following:\\n\\nProceed to the next cell and attempt to enter the sheet_name value,\\n\\nor\\n\\nExport the file as a csv, start from the beginning of the python notebook,\\nchange the file name, and rerun all cells.\\nBe sure to specify the file type as csv in the cell below when you run it.')\n",
    "    else:\n",
    "        print('Please click the next cell, and press Run.')\n",
    "        print('Here is a preview of the data: \\n\\n##StartPreview:\\n')\n",
    "        pdHeadersRowsCol1 = list(excelSheetName.iloc[1:10:,0])\n",
    "        for h in pdHeadersRowsCol1:\n",
    "            m = re.search('Animal ID', h)\n",
    "            if m:\n",
    "                processedPivotTable = True\n",
    "            print(h + '\\n')\n",
    "        print('##EndPreview\\n\\nData Type is ')\n",
    "        if processedPivotTable:\n",
    "            print('Processed Pivot Table')\n",
    "        else:\n",
    "            print('raw pivot table')\n",
    "        readyToProceed = True\n",
    "else:\n",
    "    print('No pivot table was found. Please do the following: \\n1) check the filename and rerun the previous cell\\n2) if you have already rerun the previous cell and are seeing this message again, \\nproceed to the next cell and enter information for at least one of the following: ')\n",
    "    print('\\tfile_type: enter \\'csv\\' or \\'excel\\', depending on the file.')\n",
    "    print('\\tsheet_name: enter the sheet name for the excel spreadsheet or csv file')\n",
    "    print('Then run the next two cells.')\n",
    "\n",
    "genotypeList_training = openFileAsList('alleles_parsed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_type = '' # must be left blank, 'csv' for a csv file, or 'excel' for an excel file type\n",
    "sheet_name = '' # must have the excel sheet name with the pivot table, or be blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2\n",
    "* This step will reformat the Excel data into a format that can be used by the Machine Learning Classifier.\n",
    "* Click the next cell, and then click the Run button at the top.\n",
    "* If you do not see any error messages, and you see the output 'Everything looks good!', then proceed to Step 3.  Otherwise, contact John for assistance with Step 2\n",
    "  * Note from John: In the Beta build 1.2 this step will be modified to not require intervention from me if something goes wrong.\n",
    "* If you see a warning message, it is still usually ok to proceed, but you should make a note of the warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLabeledValues(l):\n",
    "    r = []\n",
    "    for i in l:\n",
    "        iSplit = i.split('-')\n",
    "        iSplitSorted = sorted(iSplit)\n",
    "        iSplitSortdString = '-'.join(iSplitSorted)\n",
    "        if iSplit[0] == iSplit[1]:\n",
    "            r.append(iSplit[0])\n",
    "        else:\n",
    "            r.append(iSplitSortdString)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything looks good!\n"
     ]
    }
   ],
   "source": [
    "v = findColumnIdxStartStop(excelSheetName)\n",
    "if v[0] == -1:\n",
    "    print('Error!  check dataframe!')\n",
    "col1List = list(excelSheetName['Sample Sheet #'])\n",
    "skipRows, mamuA_rows, mamuB_rows, genotypeList_unparsed = parsePandasDfRows(col1List)\n",
    "genotypeList_parsed = parseGenotypeList(genotypeList_unparsed)\n",
    "genotypeListTuple = scanAndUpdateGenotypeList(genotypeList_parsed, genotypeList_training)\n",
    "# final step: parse through genotypeListTuple, and remove any rows that correspond to (0, name)\n",
    "genotypeListFiltered = list(filter(lambda x: x[0] != 0, genotypeListTuple))\n",
    "genotypeListTupleX, genotypeListTupleY = zip(*genotypeListTuple) # zip iterable, unpacks tuple\n",
    "genotypeListSkip = [x for (x,y) in enumerate(genotypeListTupleX) if y == 0]\n",
    "genotypeIntsTuple, genotypeListFromTuple = zip(*genotypeListFiltered) \n",
    "genotypeList = list(genotypeListFromTuple)\n",
    "skipMamuRows = mamuA_rows + mamuB_rows + skipRows + genotypeListSkip\n",
    "skipMamuRows.sort()\n",
    "parsedMamuIndices = parseIdxForMamuAMamuB(genotypeList)\n",
    "parsedMamuAIndices = parsedMamuIndices[0]\n",
    "parsedMamuBIndices = parsedMamuIndices[1]\n",
    "\n",
    "# print(genotypeList)\n",
    "rStart = v[0]\n",
    "rStop = v[1] + 1 \n",
    "rRange = rStop - rStart\n",
    "mamuA_alleleList = []\n",
    "mamuB_alleleList = []\n",
    "alleleDF_MamuA = None\n",
    "alleleDF_MamuB = None\n",
    "for x in range(rStart, rStop):\n",
    "    mamu_genotypes_oneHot = []\n",
    "    dfValue = excelSheetName.iloc[:,x] # verify syntax for rows and columns, and index not lookup\n",
    "    dfValue_MamuA_1 = dfValue.iloc[int(mamuA_rows[0])]\n",
    "    dfValue_MamuA_2 = dfValue.iloc[int(mamuA_rows[1])]\n",
    "    dfValue_MamuB_1 = dfValue.iloc[int(mamuB_rows[0])]\n",
    "    dfValue_MamuB_2 = dfValue.iloc[int(mamuB_rows[1])]\n",
    "    pdDict_MamuA = dict()\n",
    "    pdDict_MamuB = dict()\n",
    "    for idx, row in dfValue.iteritems():\n",
    "        if idx not in skipMamuRows:\n",
    "            try:\n",
    "                if math.isnan(row):\n",
    "                    mamu_genotypes_oneHot.append(0)\n",
    "                    continue\n",
    "                else:\n",
    "                    mamu_genotypes_oneHot.append(1)\n",
    "            except TypeError:\n",
    "                mamu_genotypes_oneHot.append(1)\n",
    "    pdDictKey_MamuA = str(dfValue_MamuA_1) + '-' + str(dfValue_MamuA_2)\n",
    "    pdDictKey_MamuB = str(dfValue_MamuB_1) + '-' + str(dfValue_MamuB_2)\n",
    "    pdDict_MamuA[pdDictKey_MamuA] = mamu_genotypes_oneHot\n",
    "    pdDict_MamuB[pdDictKey_MamuB] = mamu_genotypes_oneHot\n",
    "    alleleDF_MamuA = dataToPandasOneHot(pdDict_MamuA, alleleDF_MamuA, genotypeList)\n",
    "    alleleDF_MamuB = dataToPandasOneHot(pdDict_MamuB, alleleDF_MamuB, genotypeList)\n",
    "alleleDF_MamuA_parsed = alleleDF_MamuA.iloc[parsedMamuAIndices,:]\n",
    "alleleDF_MamuB_parsed = alleleDF_MamuB.iloc[parsedMamuBIndices,:]\n",
    "alleleDF_MamuA_parsedList = []\n",
    "alleleDF_MamuA_listLabels_Y = []\n",
    "for i in range(0, rRange):\n",
    "    dfColAsSeries = alleleDF_MamuA_parsed.iloc[:,i]\n",
    "    if dfColAsSeries.name == 'nan-nan':\n",
    "        continue\n",
    "    dfColAsList = dfColAsSeries.tolist()\n",
    "    alleleDF_MamuA_parsedList.append(dfColAsList)\n",
    "    alleleDF_MamuA_listLabels_Y.append(dfColAsSeries.name)\n",
    "alleleDF_MamuA_listedLabels_Y = parseLabeledValues(alleleDF_MamuA_listLabels_Y)\n",
    "alleleDF_MamuA_parsedNP = np.array(alleleDF_MamuA_parsedList)\n",
    "if not alleleDF_MamuA_parsed.empty and not alleleDF_MamuB_parsed.empty:\n",
    "    print('Everything looks good!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remaining steps todo before release of beta 1_0:\n",
    "# write function to insert results into pivot table\n",
    "# write logic to handle no haplotype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3\n",
    "* If you did not encounter any errors previously, or the output did not direct you to stop, then proceed with the analysis.\n",
    "* You can either click this cell, select the Cell menu above, and then select 'Run All below', or click the cells after this one and individually click 'Run' for each one\n",
    "* The analysis within this python notebook will do the following:\n",
    "  * parse out genotype ID's for the MHC-A, and MHC-B data, \n",
    "  * use a pre-trained model for a Machine Learning Classifier to predict the Haplotype for each sample for MHC-A and MHC-B\n",
    "  * add the predicted values back into the pivot table (or create a new pivot table)\n",
    "  * output the resulting pivot table\n",
    "* Note that for beta 1.x builds, only MHC-A and MHC-B haplotypes will be predicted.  All other haplotypes will be passed to the researcher for analysis.  A researcher should also verify the output pivot table from the Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.base import clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def lambdaFunc(v):\n",
    "    return int(v)\n",
    "\n",
    "def openAndParse(f):\n",
    "    listValue = []\n",
    "    headerLine = ''\n",
    "    start = True\n",
    "    with open(f) as fOpen:\n",
    "        for i in fOpen:\n",
    "            if start:\n",
    "                iLine = i.split(',')\n",
    "                iLine = iLine.pop(0)\n",
    "                headerLine = ','.join(iLine)\n",
    "                start = False\n",
    "            else:\n",
    "                i = i.rstrip('\\n')\n",
    "                iSplit = i.split(',')\n",
    "                iSplitInt = list(map(lambdaFunc, iSplit[1:]))\n",
    "                # listValue.append((iSplit[0], iSplit[1:]))\n",
    "                listValue.append((iSplit[0], iSplitInt))\n",
    "    return (headerLine, listValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingValues = openAndParse('allele_df-trainingSet-HapA.csv')\n",
    "testingValues = openAndParse('allele_df-testingSet-HapA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.empty([len(trainingValues[1]), len(trainingValues[1][1][1])], dtype=int)\n",
    "Y_train = np.empty([len(trainingValues[1])], dtype=int)\n",
    "X_test = np.empty([len(testingValues[1]), len(testingValues[1][1][1])], dtype=int)\n",
    "Y_test = np.empty([len(testingValues[1])], dtype=int)\n",
    "trainingValueFromTuple = trainingValues[1]\n",
    "testingValueFromTuple = testingValues[1]\n",
    "lookupTableBecauseNumpy = []\n",
    "lookupTableBecauseNumpyTesting = []\n",
    "stopRange = len(trainingValueFromTuple)\n",
    "for x in range(0, stopRange):\n",
    "    aValue_x = trainingValueFromTuple[x][1]\n",
    "    aValue_y = trainingValueFromTuple[x][0]\n",
    "    X_train[x] = aValue_x\n",
    "    lookupTableBecauseNumpy.append(aValue_y)\n",
    "    Y_train[x] = np.array(x, dtype=int)\n",
    "stopRange = len(testingValueFromTuple)\n",
    "for x in range(0, stopRange):\n",
    "    aValue_x = testingValueFromTuple[x][1]\n",
    "    aValue_y = testingValueFromTuple[x][0]\n",
    "    X_test[x] = aValue_x\n",
    "    lookupTableBecauseNumpyTesting.append(aValue_y)\n",
    "    Y_test[x] = np.array(x, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Developer Comments JRC\n",
    "# PRAGMA MARK: Critical Dev comment\n",
    "\n",
    "A few comments here.\n",
    "At a basic level, the training set creates a correlation between the one-hot vector of values \n",
    "that are fed to it (and was created in this case from the genotypes), with correspinding labeling.  \n",
    "The labeling is important, otherwise the output would be binary, or a series of integers.  Additionally, labeling can be used in downstream\n",
    "tasks and more complex machine learning tasks.\n",
    "\n",
    "There are several algorithms to use for training and prediction, which are too complex to explain here, \n",
    "but will be explained and discussed in later developer comments.\n",
    "The testing set then takes a set of data, which must be formatted as a one-hot vector with corresponding labeling, \n",
    "and then predicts how the test dataset will match to the training dataset.  \n",
    "\n",
    "Initially, testing and training data were from the same pooled data, so creating testing and training data followed similar methods.\n",
    "\n",
    "To use input data, the steps above--commented out now in the dev version, removed in the beta version--needed to be\n",
    "modified such that the input data was predicted to have set values, and then those values were matched to a labeling table.\n",
    "\n",
    "If input data was from a formatted pivot table, then a list of labels are created from the column names.  This becomes Y_test, and accuracy testing can be run.\n",
    "\n",
    "If the input data was from a raw pivot table, then the classifier makes predictions, and these predictions are mapped back to labeling created from the training data set.  \n",
    "In this case, no accuracy checks can be done, since these labels are unknown, however, as has been repeatedly stated, the output should be verified by a researcher.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!\n",
      "A008-A025\n",
      "[('A004', 0), ('A004', 0), ('A004', 0), ('A004', 0), ('A004-A025', 1), ('A002a-A004', 2), ('A002a-A004', 2), ('A004-A011', 3), ('A004-A023', 4), ('A001-A008', 5), ('A004-A023', 4), ('A002a-A006', 6), ('A004-A025', 1), ('A004', 0), ('A002a-A004', 2), ('A004-A023', 4), ('A004-A008', 7), ('A001', 8), ('A001', 8), ('A001-A004', 9), ('A004-A011', 3), ('A004-A008', 7), ('A004-A008', 7), ('A004', 0), ('A004', 0), ('A004', 0), ('A004-A023', 4), ('A001-A002a', 10), ('A004', 0), ('A001-A002a', 10), ('A004', 0), ('A002a-A008', 11), ('A004', 0), ('A001', 8), ('A001-A008', 5), ('A001-A008', 5), ('A004-A011', 3), ('A002a-A006', 6), ('A004-A008', 7), ('A001-A008', 5), ('A004-A008', 7), ('A002a-A008', 11), ('A004-A008', 7), ('A004', 0), ('A004-A023', 4), ('A004-A023', 4), ('A001-A004', 9), ('A002a-A004', 2), ('A004', 0), ('A001-A002a', 10), ('A004-A008', 7), ('A004-A008', 7), ('A004', 0), ('A002a-A004', 2), ('A001-A004', 9), ('A004', 0), ('A001-A004', 9), ('A002a-A004', 2), ('A004', 0), ('A004-A023', 4), ('A004', 0), ('A001-A002a', 10), ('A004-A023', 4), ('A004', 0), ('A004', 0), ('A001-A004', 9), ('A002a-A004', 2), ('A004-A008', 7), ('A004', 0), ('A001-A008', 5), ('A001-A008', 5), ('A004-A025', 1), ('A004', 0), ('A001-A004', 9), ('A004-A008', 7), ('A004-A011', 3), ('A004', 0), ('A004', 0), ('A004', 0), ('A004', 0), ('A004-A008', 7), ('A001-A008', 5), ('A001-A004', 9), ('A004', 0), ('A001-A004', 9), ('A002a-A004', 2), ('A004', 0)]\n"
     ]
    }
   ],
   "source": [
    "def formatTestingValues(tList, tupleList):\n",
    "    r = []\n",
    "    validationInt = len(tList)\n",
    "    for v in tList:\n",
    "        for vInTuple in tupleList:\n",
    "            if v == vInTuple[0]:\n",
    "                r.append(vInTuple[1])\n",
    "                break\n",
    "    if len(r) != len(tList):\n",
    "        print('WARNING!')\n",
    "        print(r)\n",
    "        print(tList)\n",
    "        return None\n",
    "    else:\n",
    "        return r\n",
    "def valuesToTestList(valList, npArrayTupleListTraining):\n",
    "    r = []\n",
    "    matched = False\n",
    "    for v in valList:\n",
    "        for vTuple in npArrayTupleListTraining:\n",
    "            if v == vTuple[1]:\n",
    "                matched = True\n",
    "                r.append(vTuple[0])\n",
    "                break\n",
    "            # if v == vTuple[0]:\n",
    "                # matched = True\n",
    "                # r.append(vTuple[1])\n",
    "                # break\n",
    "        if matched:\n",
    "            matched = False\n",
    "            continue\n",
    "        else:\n",
    "            print('WARNING!')\n",
    "            print(v)\n",
    "            print(npArrayTupleListTraining)\n",
    "            return None\n",
    "    return r\n",
    "        \n",
    "    \n",
    "def valuesToIntList(valList):\n",
    "    listedValues = []\n",
    "    ct = 0\n",
    "    l_strings = []\n",
    "    l_ints = []\n",
    "    l_strings_r = []\n",
    "    l_ints_r = []\n",
    "    npArrayList = []\n",
    "    for i in valList:\n",
    "        if i not in l_strings:\n",
    "            l_strings_r.append(i)\n",
    "            l_ints_r.append(ct)\n",
    "            l_strings.append(i)\n",
    "            l_ints.append(ct)\n",
    "            npArrayList.append(ct)\n",
    "            ct += 1\n",
    "        else:\n",
    "            v_i = l_strings.index(i)\n",
    "            v_i_string = l_strings[v_i]\n",
    "            v_ct = l_ints[v_i]\n",
    "            l_strings_r.append(v_i_string)\n",
    "            l_ints_r.append(v_ct)\n",
    "            npArrayList.append(v_ct)\n",
    "    npArrayTupleList = list(zip(l_strings_r,l_ints_r))\n",
    "    return (npArrayList, npArrayTupleList)\n",
    "\n",
    "parsedTrainingResults = valuesToIntList(lookupTableBecauseNumpy)\n",
    "Y_train = np.array(parsedTrainingResults[0])\n",
    "parsedTestingResults = valuesToIntList(lookupTableBecauseNumpyTesting)\n",
    "Y_testList = valuesToTestList(alleleDF_MamuA_listedLabels_Y, parsedTrainingResults[1])\n",
    "Y_test = np.array(Y_testList)\n",
    "# print(Y_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if processedPivotTable:\n",
    "    if processedPivotTableList:\n",
    "        print() # run function\n",
    "    else:\n",
    "        print() # # throw warning, run prediction function\n",
    "else:\n",
    "    print() # run prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(None, dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-295a22f8cfec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# forest_clf_pred = forest_clf.predict(X_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_knn_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m# This accuracy score strictly tests the model, and may be subject to overfitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 119\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(None, dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "param_grid = [{'weights': [\"uniform\", \"distance\"]}]\n",
    "\n",
    "# KNearestNeighbors was used, but a different library may be applied later\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1, weights='distance', n_neighbors=4)\n",
    "knn_clf.fit(X_train, Y_train)\n",
    "\n",
    "y_knn_pred = knn_clf.predict(X_test)\n",
    "\n",
    "# forest_clf_pred = forest_clf.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test, y_knn_pred)\n",
    "# This accuracy score strictly tests the model, and may be subject to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  8 10  9  9 10  0  0  7  4  0  0  4  5  7  7  6  2  0  2  0]\n",
      "[10  8 10  9  9 10  0  0  7  4  0  0  4  5  7  7  6  2  0  2  0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "forest_clf.fit(X_train, Y_train)\n",
    "forest_clf_pred = forest_clf.predict(X_test)\n",
    "\n",
    "accuracy_score(Y_test, forest_clf_pred)\n",
    "print(forest_clf_pred)\n",
    "print(Y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thor/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n",
      "/Users/thor/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.84782609,  0.92682927])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "cross_val_score(sgd_clf, X_train, Y_train, cv=2, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
